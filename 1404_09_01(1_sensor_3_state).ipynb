{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SahelKherad/3-story-benchmark-transformer/blob/main/1404_09_01(1_sensor_3_state).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JzyYbQohztXG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import scipy.io\n",
        "from scipy.io.matlab._mio5_params import mat_struct\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEjAYmOPAbYE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84ltkG1zAdmw"
      },
      "outputs": [],
      "source": [
        "!ls \"/content/drive/My Drive/ASCE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOPozofZA9NS"
      },
      "outputs": [],
      "source": [
        "def load_mat(path):\n",
        "    mat = scipy.io.loadmat(path, squeeze_me=True, struct_as_record=False)\n",
        "    raw = mat.get('dasy')\n",
        "    if isinstance(raw, mat_struct):\n",
        "        channels = []\n",
        "        for f in raw._fieldnames:\n",
        "            arr = getattr(raw, f)\n",
        "            channels.append(np.asarray(arr).reshape(-1))\n",
        "        data = np.stack(channels, axis=1)  # (n_samples, n_channels)\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected MAT structure. 'dasy' not a struct.\")\n",
        "    return data  # float64 by default\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/ASCE/'\n",
        "state_files = {\n",
        "    1: 'shm01a.mat', 2: 'shm02a.mat', 3: 'shm03a.mat',\n",
        "    4: 'shm04a.mat', 5: 'shm05a.mat', 6: 'shm06a.mat',\n",
        "    7: 'shm07a.mat', 8: 'shm08a.mat', 9: 'shm09a.mat',\n",
        "}\n",
        "# label_to_class_id: 1..9 -> 0..8\n",
        "label_to_cid = {lab: i for i, lab in enumerate(sorted(state_files.keys()))}\n",
        "\n",
        "scenario_data = {}   # cid -> np.ndarray [N, C]\n",
        "for lab, fname in state_files.items():\n",
        "    x = load_mat(os.path.join(base_dir, fname)).astype(np.float32)\n",
        "    scenario_data[label_to_cid[lab]] = x\n",
        "print(scenario_data[0][:,:].shape)\n",
        "print(scenario_data[0][:,4].shape)\n",
        "print(scenario_data[4][:,4].shape)\n",
        "    # print(fname, x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "win = 512\n",
        "stride = 128"
      ],
      "metadata": {
        "id": "X-4vPwZ9Qigo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "healthy_data = scenario_data[0][:,4]\n",
        "damaged_data_1 = scenario_data[7][:,4]\n",
        "damaged_data_2 = scenario_data[8][:,4]\n",
        "print(healthy_data.shape)\n",
        "print(damaged_data_1.shape)\n",
        "print(damaged_data_2.shape)"
      ],
      "metadata": {
        "id": "aNl3tzv1Fj63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_train = 0.6\n",
        "n_val = 0.2\n",
        "\n",
        "m_0 = int(n_train * len(healthy_data))\n",
        "l_0 = int(n_val * len(healthy_data))\n",
        "\n",
        "m_1 = int(n_train * len(damaged_data_1))\n",
        "l_1 = int(n_val * len(damaged_data_1))\n",
        "\n",
        "m_2 = int(n_train * len(damaged_data_2))\n",
        "l_2 = int(n_val * len(damaged_data_2))\n",
        "\n",
        "X_train_0 = healthy_data[0:m_0]\n",
        "X_val_0 = healthy_data[m_0 :m_0+l_0]\n",
        "X_test_0 = healthy_data[m_0+l_0:]\n",
        "\n",
        "X_train_1 = damaged_data_1[0:m_1]\n",
        "X_val_1 = damaged_data_1[m_1 :m_1+l_1]\n",
        "X_test_1 = damaged_data_1[m_1+l_1:]\n",
        "\n",
        "X_train_2 = damaged_data_2[0:m_2]\n",
        "X_val_2 = damaged_data_2[m_2 :m_2+l_2]\n",
        "X_test_2 = damaged_data_2[m_2+l_2:]"
      ],
      "metadata": {
        "id": "No19Av0aGGke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('x_train_o.shape:', X_train_0.shape)\n",
        "print('x_val_o.shape:', X_val_0.shape)\n",
        "print('x_test_o.shape:', X_test_0.shape)\n",
        "print('x_train_1.shape:', X_train_1.shape)\n",
        "print('x_train_2.shape:', X_train_2.shape)"
      ],
      "metadata": {
        "id": "iaHN_fqBGGft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "con = np.concatenate([X_train_0,X_train_1, X_train_2])\n",
        "print(con.shape)\n",
        "mu = con.mean(axis=(0) , keepdims = True)\n",
        "print('mu:',mu)\n",
        "std = con.std(axis=(0), keepdims=True) + 1e-6"
      ],
      "metadata": {
        "id": "XUigudXtGTof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_0 = (X_train_0 - mu) / std\n",
        "X_val_0   = (X_val_0   - mu) / std\n",
        "X_test_0  = (X_test_0  - mu) / std\n",
        "\n",
        "X_train_1 = (X_train_1 - mu) / std\n",
        "X_val_1   = (X_val_1   - mu) / std\n",
        "X_test_1  = (X_test_1  - mu) / std\n",
        "\n",
        "X_train_2 = (X_train_2 - mu) / std\n",
        "X_val_2   = (X_val_2   - mu) / std\n",
        "X_test_2  = (X_test_2  - mu) / std"
      ],
      "metadata": {
        "id": "V71gTlEZGGar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_0.shape)\n",
        "print(X_val_0.shape)\n",
        "print(X_test_0.shape)"
      ],
      "metadata": {
        "id": "ad4Jc54ldQOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_windows_with_stride(data, window_length, stride):\n",
        "\n",
        "    num_frames = int((len(data) - window_length) // (stride + 1e-8) + 1)\n",
        "    print(num_frames)\n",
        "\n",
        "    if num_frames <= 0:\n",
        "        # اگر طول داده برای یک پنجره هم کافی نبود\n",
        "        return np.empty((0, window_length), dtype=data.dtype)\n",
        "\n",
        "    # 2. ایجاد اندیس‌های شروع برای هر پنجره\n",
        "    # شروع: 0، پایان: اندیس لازم برای آخرین پنجره کامل\n",
        "    start_indices = np.arange(num_frames) * stride\n",
        "\n",
        "    # 3. ایجاد یک ماتریس اندیس برای استخراج داده ها\n",
        "    # این کار از حلقه زدن کندتر جلوگیری می کند و کل فرآیند را برداری (Vectorized) می کند\n",
        "    window_indices = start_indices[:, None] + np.arange(window_length)\n",
        "\n",
        "    # 4. استخراج داده ها با استفاده از اندیس‌های محاسبه شده\n",
        "    windows = data[window_indices]\n",
        "\n",
        "    return windows"
      ],
      "metadata": {
        "id": "Ho4EyqjQDezv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "win = 512\n",
        "stride = 512"
      ],
      "metadata": {
        "id": "LgCOCzHEdpbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Make windows per class\n",
        "X_train_0 = make_windows_with_stride(X_train_0, win, stride)\n",
        "X_val_0   = make_windows_with_stride(X_val_0, win, stride)\n",
        "X_test_0  = make_windows_with_stride(X_test_0, win, stride)\n",
        "\n",
        "y_0 = np.zeros((X_train_0.shape[0]+X_val_0.shape[0]+X_test_0.shape[0]), dtype=np.int64)\n",
        "\n",
        "print('Train_0 = \\n')\n",
        "print(X_train_0.shape)\n",
        "print(X_val_0.shape)\n",
        "print(X_test_0.shape)\n",
        "print(y_0.shape)\n",
        "\n",
        "X_train_1 = make_windows_with_stride(X_train_1, win, stride)\n",
        "X_val_1 = make_windows_with_stride(X_val_1, win, stride)\n",
        "X_test_1 = make_windows_with_stride(X_test_1, win, stride)\n",
        "\n",
        "y_1 = np.ones((X_train_1.shape[0]+X_val_1.shape[0]+X_test_1.shape[0]), dtype=np.int64)\n",
        "\n",
        "print('\\n Train_1 = \\n')\n",
        "print(X_train_1.shape)\n",
        "print(X_val_1.shape)\n",
        "print(X_test_1.shape)\n",
        "print(y_1.shape)\n",
        "\n",
        "X_train_2 = make_windows_with_stride(X_train_2, win, stride)\n",
        "X_val_2 = make_windows_with_stride(X_val_2, win, stride)\n",
        "X_test_2 = make_windows_with_stride(X_test_2, win, stride)\n",
        "\n",
        "y_2 = 2 * np.ones((X_train_2.shape[0]+X_val_2.shape[0]+X_test_2.shape[0]), dtype=np.int64)\n",
        "\n",
        "print('\\n Train_1 = \\n')\n",
        "print(X_train_2.shape)\n",
        "print(X_val_2.shape)\n",
        "print(X_test_2.shape)\n",
        "print(y_2.shape)"
      ],
      "metadata": {
        "id": "BuJ8Wpy9VtcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHRDgV6D57Ac"
      },
      "outputs": [],
      "source": [
        "# def make_windows(data, window_length, drop_last : bool = True):\n",
        "#     # Calculate how many full frames can be created\n",
        "#     num_frames = len(data) // window_length\n",
        "#     # print(num_frames)\n",
        "#     # Trim the data to be a perfect multiple of the frame length\n",
        "#     trimmed_data = data[:num_frames * window_length]\n",
        "\n",
        "#     # Reshape the data into a 2D array of frames\n",
        "#     windows = trimmed_data.reshape((num_frames, window_length))\n",
        "#     # return trimmed_data.shape , frames.shape\n",
        "#     return windows"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# win = 300\n",
        "# 2) Make windows per class\n",
        "# X_train_0 = make_windows(X_train_0, win, drop_last=True)\n",
        "# X_val_0 = make_windows(X_val_0, win, drop_last=True)\n",
        "# X_test_0 = make_windows(X_test_0, win, drop_last=True)\n",
        "\n",
        "# y_0 = np.zeros((X_train_0.shape[0]+X_val_0.shape[0]+X_test_0.shape[0]), dtype=np.int64)\n",
        "\n",
        "# print('Train_0 = \\n')\n",
        "# print(X_train_0.shape)\n",
        "# print(X_val_0.shape)\n",
        "# print(X_test_0.shape)\n",
        "# print(y_0.shape)\n",
        "\n",
        "# X_train_1 = make_windows(X_train_1, win, drop_last=True)\n",
        "# X_val_1 = make_windows(X_val_1, win, drop_last=True)\n",
        "# X_test_1 = make_windows(X_test_1, win, drop_last=True)\n",
        "\n",
        "# y_1 = np.ones((X_train_1.shape[0]+X_val_1.shape[0]+X_test_1.shape[0]), dtype=np.int64)\n",
        "\n",
        "# print('\\n Train_1 = \\n')\n",
        "# print(X_train_1.shape)\n",
        "# print(X_val_1.shape)\n",
        "# print(X_test_1.shape)\n",
        "# print(y_1.shape)"
      ],
      "metadata": {
        "id": "ofjlMTSxGhED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.vstack([X_train_0, X_train_1, X_train_2])[:, :, None]  # [N, 5, 1]\n",
        "print(X_train.shape)\n",
        "\n",
        "y_train = np.concatenate([y_0[0:int(len(X_train_0))], y_1[0:int(len(X_train_1))], y_2[0:int(len(X_train_2))]])             # [N]\n",
        "print(y_train)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "hXkqupXzGqem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(42)\n",
        "perm = rng.permutation(len(X_train))\n",
        "\n",
        "X_train = X_train[perm]\n",
        "y_train = y_train[perm]\n",
        "print(y_train)"
      ],
      "metadata": {
        "id": "xCQQkDlH8ZYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = np.vstack([X_val_0, X_val_1, X_val_2], )[:, :, None]  # [N, 5, 1]\n",
        "print(X_val.shape)\n",
        "\n",
        "y_val = np.concatenate([y_0[int(len(X_train_0)):int(len(X_train_0) + len(X_val_0))], y_1[int(len(X_train_1)):int(len(X_train_1) + len(X_val_1))], y_2[int(len(X_train_2)):int(len(X_train_2) + len(X_val_2))]])             # [N]\n",
        "print(y_val)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "id": "BdK647bqG2zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test =  np.vstack([X_test_0, X_test_1, X_test_2])[:, :, None]  # [N, 5, 1]\n",
        "print(X_test.shape)\n",
        "\n",
        "y_test = np.concatenate([y_0[int(len(X_train_0) + len(X_val_0)):], y_1[int(len(X_train_1) + len(X_val_1)):], y_2[int(len(X_train_2) + len(X_val_2)):]], axis=0)             # [N]\n",
        "print(y_test)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "5tnMiR0IHGBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Torch tensors\n",
        "X_train = torch.from_numpy(X_train).float()  # float32\n",
        "y_train = torch.from_numpy(y_train).long()  # int64\n",
        "X_val   = torch.from_numpy(X_val).float()\n",
        "y_val   = torch.from_numpy(y_val).long()\n",
        "X_test  = torch.from_numpy(X_test).float()\n",
        "y_test  = torch.from_numpy(y_test).long()"
      ],
      "metadata": {
        "id": "EyfNxcS0OW3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "ITFAlRmJOWdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn18DVVKq76e"
      },
      "outputs": [],
      "source": [
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, X: torch.Tensor, y: torch.Tensor):\n",
        "        self.X = X  # [N, seq_len, 1]\n",
        "        self.y = y  # [N]\n",
        "    def __len__(self): return self.y.shape[0]\n",
        "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "\n",
        "train_ds = SeqDataset(X_train, y_train)\n",
        "# print(X_train.shape)\n",
        "# print(train_ds[0])\n",
        "# print(train_ds[1])\n",
        "# print(train_ds[2])\n",
        "val_ds   = SeqDataset(X_val,   y_val)\n",
        "test_ds  = SeqDataset(X_test,  y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ma0uxTa8nsD"
      },
      "outputs": [],
      "source": [
        "# 6) DataLoaders: shuffle only the training set\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILHeIaNA_Y1t"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    ys_tr = torch.cat([yb.view(-1) for _, yb in train_loader], dim=0).cpu()\n",
        "    ys_va = torch.cat([yb.view(-1) for _, yb in val_loader], dim=0).cpu()\n",
        "\n",
        "print(\"train label set/counts:\", ys_tr.unique(return_counts=True))\n",
        "print(\"val   label set/counts:\", ys_va.unique(return_counts=True))\n",
        "print(\"train min/max:\", ys_tr.min().item(), ys_tr.max().item())\n",
        "print(\"val   min/max:\", ys_va.min().item(), ys_va.max().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue57o5HKq7fi"
      },
      "outputs": [],
      "source": [
        "# 7) Example: adapt a batch for a Transformer [seq_len, batch, d_model]\n",
        "xb, yb = next(iter(train_loader))   # xb: [B, 5, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAd745oVq7Sd"
      },
      "outputs": [],
      "source": [
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Learnable positional encoding added to token embeddings.\n",
        "    Shape stored: [1, max_len, d_model]\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int):\n",
        "        super().__init__()\n",
        "        self.pe = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
        "        print(self.pe)\n",
        "        nn.init.trunc_normal_(self.pe, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, d_model]\n",
        "        T = x.size(1)\n",
        "        return x + self.pe[:, :T, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBe4P6n8puIs"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, d_model=256, nhead=4, num_layers=3, d_ff=256, seq_len= X_train.shape[1], num_classes=3, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.inp = nn.Linear(1, d_model)                            # project feature_dim=1 -> d_model\n",
        "        self.pos = LearnablePositionalEncoding(d_model, seq_len)    # learnable PE\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead,\n",
        "            dim_feedforward=d_ff, dropout=dropout,\n",
        "            batch_first=True                                        # so we keep [B, T, d]\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, 1]\n",
        "        x = self.inp(x)                  # [B, T, d]\n",
        "        x = self.pos(x)                  # add learnable PE\n",
        "        x = self.encoder(x)              # [B, T, d]\n",
        "        x = self.norm(x)\n",
        "        x = x.mean(dim=1)                # mean pool over time\n",
        "\n",
        "        logits = self.head(x)            # [B, C]\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfewTjRosCAY"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seq_len = X_train.shape[1]\n",
        "num_classes = 2\n",
        "model = TimeSeriesTransformer(\n",
        "    d_model=64,\n",
        "    nhead=2,\n",
        "    num_layers=4,\n",
        "    d_ff=128,\n",
        "    seq_len=seq_len ,\n",
        "    num_classes=3,\n",
        "    dropout=0.1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2149a252"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model.to(device) # Move the model to the selected device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp_VLwP7sB7K"
      },
      "outputs": [],
      "source": [
        "# Optional class weights for CE or focal alpha; compute from training labels\n",
        "class_counts = torch.bincount(y_train)\n",
        "class_weights = (class_counts.sum() / (class_counts.float().clamp(min=1)))  # inverse freq\n",
        "class_weights = class_weights / class_weights.sum() * num_classes           # normalize\n",
        "class_weights = class_weights.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xc9Wh7DsB0e"
      },
      "outputs": [],
      "source": [
        "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "# # optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0)\n",
        "# # loss_fn = nn.CrossEntropyLoss()\n",
        "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "# scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZbtuUUR-_dk"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "tr_losses, tr_accs, val_losses, val_accs = [], [], [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "     model.train()\n",
        "     total_loss, total_samples, correct_predictions = 0, 0, 0\n",
        "     for xb, yb in train_loader:\n",
        "        xb , yb = xb.to(device) , yb.to(device).long()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(xb)\n",
        "        # print('output',output)\n",
        "        # print('max',torch.max(output, 1))\n",
        "        # print('yb',yb)\n",
        "\n",
        "        loss = loss_fn(output , yb)\n",
        "        # print('loss',loss)\n",
        "        # optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        # print(loss.item())\n",
        "        # print(xb.size(0))\n",
        "        # print('total:',total_loss)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        pred = output.argmax(dim=1) # the index of the maximum value for each column\n",
        "        # print('p',predicted)\n",
        "        # print('real',yb)\n",
        "        total_samples += xb.size(0)\n",
        "        # print(total_samples)\n",
        "        correct_predictions += (pred == yb).sum().item()\n",
        "        # print(correct_predictions)\n",
        "\n",
        "     epoch_loss = total_loss / total_samples\n",
        "     epoch_accuracy = correct_predictions / total_samples\n",
        "     tr_losses.append(epoch_loss)\n",
        "     tr_accs.append(epoch_accuracy)\n",
        "\n",
        "    #  print(f\"Epoch {epoch+1}/{num_epochs}: \" f\"Train Loss: {epoch_loss:.4f}, \"f\"Train Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "     model.eval()\n",
        "     val_correct_predictions , val_total_samples , val_total_loss = 0 , 0 , 0\n",
        "\n",
        "     with torch.no_grad():\n",
        "          for xb, yb in val_loader:\n",
        "              xb, yb = xb.to(device), yb.to(device).long()\n",
        "              output = model(xb)\n",
        "              val_loss = loss_fn(output,yb)\n",
        "              val_total_loss += val_loss.item() * xb.size(0)\n",
        "              pred = output.argmax(dim=1)\n",
        "              val_total_samples += xb.size(0)\n",
        "              val_correct_predictions += (pred == yb).sum().item()\n",
        "\n",
        "     val_epoch_loss = val_total_loss / val_total_samples\n",
        "     val_epoch_accuracy = val_correct_predictions / val_total_samples\n",
        "    #  print(    f\"Validation Loss: {val_epoch_loss:.4f}, \"f\"Validation Accuracy: {val_epoch_accuracy:.4f}\\n\")\n",
        "     val_losses.append(val_epoch_loss)\n",
        "     val_accs.append(val_epoch_accuracy)\n",
        "     # 3) Step the scheduler once per epoch, after validation\n",
        "    #  scheduler.step()\n",
        "\n",
        "     # Optional: print current lr to confirm schedule\n",
        "     current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "     print(f\"Epoch {epoch+1}/{num_epochs}: \" f\"Train Loss: {epoch_loss:.4f}, \"f\"Train Accuracy: {epoch_accuracy:.4f} | \"f\"Validation Loss: {val_epoch_loss:.4f}, \"f\"Validation Accuracy: {val_epoch_accuracy:.4f}\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "val_probs = []\n",
        "val_true  = []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in val_loader:\n",
        "        out = model(xb.to(device))\n",
        "        val_probs.append(F.softmax(out, dim=1).max(1).values.cpu())\n",
        "        val_true.append(yb)\n",
        "val_probs = torch.cat(val_probs)\n",
        "print('Val avg max-softmax:', val_probs.mean().item())"
      ],
      "metadata": {
        "id": "Rv6VrEcbOmsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "all_pred, all_true = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in val_loader:\n",
        "        out = model(xb.to(device))\n",
        "        all_pred.append(out.argmax(1).cpu().numpy())\n",
        "        all_true.append(yb.numpy())\n",
        "cm = confusion_matrix(np.concatenate(all_true), np.concatenate(all_pred))\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "O2Kz8WWGOxP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KklICuBQ_MlM"
      },
      "outputs": [],
      "source": [
        "epoch = np.arange(0,num_epochs , 1)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(epoch, tr_losses , label=\"Train_loss\", c='b')\n",
        "plt.plot(epoch, val_losses, label=\"val_losss\", c='r')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhEnH9lVsBis"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_true, y_pred, y_score = [], [], []\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in val_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device).long()   # targets should be int for CE\n",
        "        logits = model(xb)                             # [B,2]\n",
        "\n",
        "        # 1) Probability of class 1\n",
        "        probs  = torch.softmax(logits, dim=1)          # [B,2]\n",
        "        score1 = probs[:, 1]                           # [B]\n",
        "\n",
        "        # 2) Predicted class index\n",
        "        preds  = logits.argmax(dim=1)                  # [B]\n",
        "\n",
        "        # Save results\n",
        "        y_true.extend(yb.cpu().tolist())\n",
        "        y_pred.extend(preds.cpu().tolist())\n",
        "        y_score.extend(score1.cpu().tolist())\n",
        "\n",
        "# 1) Confusion Matrix\n",
        "ConfusionMatrixDisplay.from_predictions(y_true, y_pred, cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix (Validation)\")\n",
        "plt.show()\n",
        "\n",
        "# 2) ROC\n",
        "RocCurveDisplay.from_predictions(y_true, y_score)\n",
        "plt.title(\"ROC Curve (Validation)\")\n",
        "plt.show()\n",
        "\n",
        "# 3) Precision–Recall\n",
        "PrecisionRecallDisplay.from_predictions(y_true, y_score)\n",
        "plt.title(\"Precision–Recall Curve (Validation)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZAQEGdnsi9u"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "model.eval()\n",
        "all_embeds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in val_loader:\n",
        "        xb = xb.to(device)\n",
        "        # Pass through encoder only (skip classifier head)\n",
        "        embeds = model.encoder(model.inp(xb) + model.pos(model.inp(xb)))\n",
        "        embeds = embeds.mean(dim=1)                # [B, d_model]\n",
        "\n",
        "        all_embeds.append(embeds.cpu())\n",
        "        all_labels.append(yb.cpu())\n",
        "\n",
        "# Concatenate all batches\n",
        "all_embeds = torch.cat(all_embeds, dim=0).numpy()\n",
        "all_labels = torch.cat(all_labels, dim=0).numpy()\n",
        "\n",
        "# Run t-SNE\n",
        "X_2d = TSNE(n_components=2, random_state=42).fit_transform(all_embeds)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(X_2d[:,0], X_2d[:,1], c=all_labels, cmap=\"coolwarm\", alpha=0.7)\n",
        "plt.colorbar(label=\"Class\")\n",
        "plt.title(\"t-SNE of Validation Embeddings\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP81mgqC28cb8yJzMcYb26b",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}